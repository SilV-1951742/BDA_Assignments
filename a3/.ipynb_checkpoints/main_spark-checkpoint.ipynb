{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7b45eb-b95e-4bed-b429-10717c3a0512",
   "metadata": {},
   "source": [
    "# Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764bccb-ae89-431d-8b0a-44690d69d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell spark.executor.cores=2\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import flatten\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, \n",
    "                                FloatType, DateType, IntegerType, ArrayType)\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"BDA assignment\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534ea9a-b7cf-4971-9330-6cb615863303",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbd6460-830a-445d-863b-b87edb4b0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Final, List\n",
    "#from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import islice, chain, combinations\n",
    "import argparse\n",
    "import traceback\n",
    "import bleach\n",
    "import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import pr\n",
    "import string\n",
    "import random\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6aed4f-9be9-4a14-a996-078adae3e54f",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d055241-e20d-43b1-8406-d412b5f34cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHINGLE_SIZE: Final = 5\n",
    "SAMPLES: Final = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f90724-7f77-4267-bd27-723fdef44ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class comment_tuple(NamedTuple):\n",
    "    id: int\n",
    "    #owner_id: int\n",
    "    post_type: int\n",
    "    score: int\n",
    "    text: str\n",
    "\n",
    "class shingle_set(NamedTuple):\n",
    "    id: int\n",
    "    shingles: frozenset[tuple]\n",
    "\n",
    "class similarity(NamedTuple):\n",
    "    id_set1: int\n",
    "    id_set2: int\n",
    "    similarity: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed988ef-bf90-4ec9-a288-c32a0441df13",
   "metadata": {},
   "source": [
    "# Read and clean XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc472be3-e7df-4983-ad8d-734155857e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_schema():\n",
    "    \"\"\"\n",
    "    Define the schema for the DataFrame\n",
    "    \"\"\"\n",
    "    schema_list = []\n",
    "    schema_list.append(StructField(\"Id\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"PostTypeId\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"Score\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"Body\", StringType(), True))\n",
    "    \n",
    "    return StructType(schema_list)\n",
    "\n",
    "def parse_post(rdd):\n",
    "    results = []\n",
    "    root = ET.fromstring(rdd[0])\n",
    "\n",
    "    for elem in root.findall('row'):\n",
    "        rec = []\n",
    "        #print(\"Found row\")\n",
    "        assert elem.text is None, \"The row wasn't empty\"\n",
    "        rec.append(int(elem.attrib[\"Id\"]))\n",
    "        #int(elem.attrib[\"OwnerUserId\"]),\n",
    "        #rec.append(int(elem.attrib[\"PostTypeId\"])),\n",
    "        #rec.append(int(elem.attrib[\"Score\"])),\n",
    "        rec.append(bleach.clean(elem.attrib[\"Body\"], strip=True))\n",
    "        #rec.append(elem.attrib[\"Body\"])\n",
    "\n",
    "        #elem.clear()\n",
    "        #while elem.getprevious() is not None:\n",
    "        #    del elem.getparent()[0]\n",
    "        results.append(rec)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9e592e-7f1e-40a0-8a94-95a8a8018038",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"cstheory_posts.bakxml\"\n",
    "chunksize = 1024\n",
    "\n",
    "file_rdd = spark.read.text(filename, wholetext=True).rdd\n",
    "dataset = file_rdd.flatMap(parse_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1b1d30-61e5-488f-a09e-76acb8969874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| Id|                Body|\n",
      "+---+--------------------+\n",
      "|  2|I have a dataset ...|\n",
      "|  3|A particular prog...|\n",
      "|  4|What is the follo...|\n",
      "|  5|Can the divide an...|\n",
      "|  6|Is anyone aware o...|\n",
      "|  7|In general, the q...|\n",
      "|  8|If I understand t...|\n",
      "|  9|<a href=\"http://w...|\n",
      "| 10|There was recentl...|\n",
      "| 11|Functional progra...|\n",
      "| 12|I took a class on...|\n",
      "| 13|It's possible tha...|\n",
      "| 14|Other than going ...|\n",
      "| 15|[This question ha...|\n",
      "| 16|See the <a href=\"...|\n",
      "| 17|it is often said ...|\n",
      "| 18|In one word: No.\n",
      "...|\n",
      "| 19|In short, I would...|\n",
      "| 20|It's easy to prov...|\n",
      "| 21|Any &quot;cutting...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.persist()\n",
    "schema = set_schema()\n",
    "df_ds = dataset.toDF(schema)\n",
    "df_ds.show()\n",
    "\n",
    "#df_posts = records_rdd.toDF(schema)\n",
    "#coll = records_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed4fe3-7297-4445-8ce2-32be93b42e1c",
   "metadata": {},
   "source": [
    "# Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e3fecd3-7b72-490d-b4ab-b1c8c4920cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shingler:\n",
    "    \"\"\"\n",
    "    Class that contain a tokenizer and stopwords to make shingling easier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        return True\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # text_nop = text.split()\n",
    "        text_nop = word_tokenize(text)\n",
    "        filtered_words = []\n",
    "\n",
    "        for word in text_nop:\n",
    "            if word not in self.stopwords:\n",
    "                filtered_words.append(word.lower())\n",
    "        \n",
    "        return filtered_words\n",
    "\n",
    "    #def create_shingle(self, input_comment: comment_tuple, shingle_size: int) -> frozenset[tuple]:\n",
    "    #    tokens = self.tokenize(input_comment.text)\n",
    "    #    comment_length = len(tokens)\n",
    "    #    shingles =  frozenset(tuple(tokens[i:(i + shingle_size)]) for i in range(comment_length - shingle_size + 1))\n",
    "    \n",
    "    def create_shingle(self, post: str, shingle_size: int) -> list[list]:\n",
    "        tokens = self.tokenize(post)\n",
    "        comment_length = len(tokens)\n",
    "        shingle_set = frozenset(tuple(tokens[i:(i + shingle_size - 1)]) for i in range(comment_length - shingle_size))\n",
    "        shingle_list = list(shingle_set)\n",
    "        #for elem in shingle_list:\n",
    "        #    elem.sort()\n",
    "        shingle_list.sort()\n",
    "        return list(shingle_set)\n",
    "        \n",
    "\n",
    "def shingle_map(row):\n",
    "    ds_shingler = shingler()\n",
    "    return (row[0], ds_shingler.create_shingle(row[1], SHINGLE_SIZE)\n",
    ")\n",
    "\n",
    "def set_shingle_schema():\n",
    "    \"\"\"\n",
    "    Define the schema for the DataFrame\n",
    "    \"\"\"\n",
    "    schema_list = []\n",
    "    schema_list.append(StructField(\"Id\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"PostTypeId\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"Score\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"Shingles\", ArrayType(ArrayType(StringType()), True)))\n",
    "    return StructType(schema_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855eb6f2-07ef-4ef9-b1ba-f8d3d0a9209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, [('strict', 'ordering', 'increasing', 'go'), ('9li', 'ul', 'can', 'i'), ('increasing', 'toptobottom', 'within', 'column'), ('li4', '6', '7li', 'li5'), ('can', 'i', 'improve', 'naive'), ('measured', 'comparisons', 'what', 'nd'), ('row', 'increasing', 'toptobottom', 'within'), ('nd', 'datasets', 'arbitrary', 'finite'), ('within', 'row', 'increasing', 'toptobottom'), ('3li', 'li4', '6', '7li'), ('column', 'for', 'example', 'ul'), ('i', 'know', 'i', 'strict'), ('know', 'i', 'strict', 'ordering'), ('example', 'ul', 'li1', '2'), ('ul', 'li1', '2', '3li'), ('entire', 'dataset', 'linearly', 'measured'), ('i', 'strict', 'ordering', 'increasing'), ('8', '9li', 'ul', 'can'), ('arranged', '2d', 'grid', 'i'), ('i', 'dataset', 'number', 'objects'), ('ordering', 'increasing', 'go', 'lefttoright'), ('dataset', 'linearly', 'measured', 'comparisons'), ('improve', 'naive', 'sorting', 'sort'), ('6', '7li', 'li5', '8'), ('sorting', 'sort', 'entire', 'dataset'), ('datasets', 'arbitrary', 'finite', 'datasets'), ('for', 'example', 'ul', 'li1'), ('objects', 'arranged', '2d', 'grid'), ('lefttoright', 'within', 'row', 'increasing'), ('increasing', 'go', 'lefttoright', 'within'), ('ul', 'can', 'i', 'improve'), ('what', 'nd', 'datasets', 'arbitrary'), ('li1', '2', '3li', 'li4'), ('number', 'objects', 'arranged', '2d'), ('linearly', 'measured', 'comparisons', 'what'), ('arbitrary', 'finite', 'datasets', 'subset'), ('naive', 'sorting', 'sort', 'entire'), ('2', '3li', 'li4', '6'), ('toptobottom', 'within', 'column', 'for'), ('grid', 'i', 'know', 'i'), ('within', 'column', 'for', 'example'), ('i', 'improve', 'naive', 'sorting'), ('go', 'lefttoright', 'within', 'row'), ('li5', '8', '9li', 'ul'), ('2d', 'grid', 'i', 'know'), ('sort', 'entire', 'dataset', 'linearly'), ('dataset', 'number', 'objects', 'arranged'), ('7li', 'li5', '8', '9li'), ('comparisons', 'what', 'nd', 'datasets')])\n"
     ]
    }
   ],
   "source": [
    "schema = set_shingle_schema()\n",
    "shingle_rdd = dataset.map(shingle_map).sortByKey()\n",
    "\n",
    "for elem in shingle_rdd.take(1):\n",
    "    print(elem)\n",
    "#df_shingle = shingle_rdd.toDF(schema)\n",
    "#df_shingle.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b363a0e8-480f-4bc0-9167-1b58be6e6c80",
   "metadata": {},
   "source": [
    "# MinHashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3799fdf-d96c-4f41-87fd-f40c6353a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2098660572, 74013022, 2675907171, 2481944613, 2289028529, 846582864, 1024003631, 3457496236, 958218556, 2872479854, 1385329197, 2720560315, 2596604670, 4118717270, 3831528778, 4184700433, 130382780, 4221132295, 2492231677, 1349853675, 2674011478, 4230155413, 522369175, 1349933754, 2597169981, 1045439184, 3199517916, 3020468163, 618450142, 3282454786, 4061764145, 3477766529, 3055070885, 204747729, 834378094, 4046990333, 2141660100, 2118590568, 3537961224, 168082228, 652073352, 1284401985, 2826423363, 1325707607, 3731067674, 723716237, 3333080471, 2787841228, 2113795590, 3230829213]\n",
      "[[1843969928, 746019385, 2309594694, 2229839123, 489336389, 779150137, 1446103079, 889348879, 3745936412, 4281307413, 2173665908, 1717438567, 23839969, 2145022294, 1052302893, 1118678081, 1295195691, 3851408124, 3928788718, 2370966349, 561561790, 1051546482, 4224882136, 4206633849, 1329396735, 1801554849, 1138195103, 3123692201, 1122989246, 1270364612, 1593839649, 1463959047, 4229718824, 1709163888, 33163415, 2667737317, 2446567315, 4184687507, 3396502067, 483813678, 2220062805, 290402151, 576718488, 2834691631, 2746963148, 1838059277, 3070124528, 885776293, 1650043432, 3582686598], [220792127, 2535197382, 1699971710, 1898865050, 3798071317, 2827338235, 435208719, 1570192009, 2360615333, 515231859, 643500, 4187339056, 509908782, 486802097, 3677215785, 2921827920, 3237468089, 3427794385, 4032002655, 4093886240, 2687035724, 3077734642, 1170835917, 432938307, 2306967322, 2141534378, 2906718316, 1574298930, 4232243766, 240355277, 3647978042, 2451964704, 3514275296, 2490192022, 1350263172, 1011477998, 1728226503, 2593311830, 2348203625, 2090761938, 2237486929, 2969854285, 3729324258, 2729987853, 2824894019, 4163387808, 3551421043, 4153788824, 2068844359, 3459047483]]\n"
     ]
    }
   ],
   "source": [
    "# Transform posts to characteristic matrix\n",
    "# Make feature set matrix\n",
    "# Minhash\n",
    "# Make Minhash Matrix\n",
    "# LSH\n",
    "import random\n",
    "SIGNATURE_SIZE: Final = 50\n",
    "HASH_PRIME: Final = (1 << 31) - 1\n",
    "MAX_HASH: Final = (1 << 32)\n",
    "    #sys.maxsize\n",
    "HASH_RANGE: Final = (1<< 32)\n",
    "    #sys.maxsize\n",
    "SEED: Final = 193120\n",
    "\n",
    "generator = np.random.RandomState(SEED)\n",
    "salts = [generator.randint(1, MAX_HASH) for _ in range(SIGNATURE_SIZE)]\n",
    "#permutations = [generator.randint(1, HASH_PRIME) for _ in range(SIGNATURE_SIZE)]\n",
    "\n",
    "permutations = [[generator.randint(1, MAX_HASH) for _ in range(SIGNATURE_SIZE)],\n",
    "                [generator.randint(0, MAX_HASH) for _ in range(SIGNATURE_SIZE)]]\n",
    "\n",
    "print(salts)\n",
    "print(permutations)\n",
    "    \n",
    "def min_hasher(row):\n",
    "    sig = np.full((SIGNATURE_SIZE), MAX_HASH)\n",
    "    for shingle in row[1]:\n",
    "        for i in range(SIGNATURE_SIZE):\n",
    "            a = permutations[0][i]\n",
    "            b = permutations[1][i]\n",
    "            hash_val = (a * hash(shingle) + b) % HASH_PRIME\n",
    "            sig[i] = min(hash_val, sig[i])\n",
    "    return (row[0], sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4314e1cc-f0b4-4631-824c-ad0bbfc48fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253594894\n",
      "253594894\n"
     ]
    }
   ],
   "source": [
    "a = permutations[0][1]\n",
    "b = permutations[1][1]\n",
    "\n",
    "hash_val = (a * hash(tuple([\"text\", \"word\", \"shingle\", \"advanced\"]))+ b) % HASH_PRIME\n",
    "print(hash_val)\n",
    "\n",
    "hash_val = (a * hash(tuple([\"text\", \"word\", \"shingle\", \"advanced\"]))+ b) % HASH_PRIME\n",
    "print(hash_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863a39cf-392b-4310-9950-92bb58df1890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, array([ 67594442,  16439677,  45145227,   4436395,   8985140,  14815891,\n",
      "        14899115,   3881532,  17237017,   7694314,  60357490,  55174578,\n",
      "        75800407,  43635722,  63138960,   6342748,  19097712,  15931731,\n",
      "        78768215,  68083458,   7852897, 111114588,  82267128,  40512217,\n",
      "        26979292,  50718242,  84178184,  16289025,  36023808,  31688981,\n",
      "       122955611,  14924541,  62927462,   8596550,  23602135,  17890570,\n",
      "        22790027, 121920241, 169149658,  10040349,  25961661,  24764912,\n",
      "         1862209,  82976344,  28659032,  70350364,  43287416,   7447052,\n",
      "         4478270,  20329288]))\n",
      "(3, array([  3535652,  15392411,   6836933,  32777016,   8174702,  15156597,\n",
      "        59148346,   2945670,  13288112,   9611155,    958304,  13282067,\n",
      "         2347109,  15419119,  12218610,  41357193,  32498178,   3876745,\n",
      "       116447371,  12568877,  38025127,  73541387,  13986610,  12524478,\n",
      "        55480223,  12384408,  25836070,  18757639,  40913942,   6687445,\n",
      "        14093763,  16550464,  17258077,  20848813,  41290079,  38996065,\n",
      "         7004385,  15966584,  20390261,   2167660,   7475643,   5743626,\n",
      "         3560618,   5698139,   6807553,  33316071,   4980028,  19767212,\n",
      "         5378309,  12913066]))\n",
      "(4, array([27869338, 10785588, 82613083,  9739498, 34959416, 10607296,\n",
      "       34676514, 14730014,  2859713,  7379123, 11310979,  2667804,\n",
      "       37371149, 36544230,  7413022, 15868892, 47915226, 36577649,\n",
      "       20425663,  4094064, 14625874,  8158550,   128999,  3461400,\n",
      "       23547856, 17264319,  8568040,  3963464, 55003778,  4668738,\n",
      "       20056746,  5719738, 23745301,   990808, 11111645,  2190895,\n",
      "        4207652, 31796720,  3849626, 16019289, 11893582, 62560028,\n",
      "        7846871, 11722493,  1742059, 13736792, 30701154, 14609117,\n",
      "        5360736, 24047099]))\n"
     ]
    }
   ],
   "source": [
    "hash_rdd = shingle_rdd.map(min_hasher)\n",
    "\n",
    "for elem in hash_rdd.take(3):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d745c9-7084-4756-8d7d-993612a4daf3",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a0a2172-ea98-45a5-bd56-51aef933432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bands: 10, rows 5, threshold 0.6309573444801932\n"
     ]
    }
   ],
   "source": [
    "BANDS: Final = 10\n",
    "ROWS: Final = 5\n",
    "THRESHOLD: Final = (1/BANDS) ** (1/ROWS)\n",
    "print(f\"Bands: {BANDS}, rows {ROWS}, threshold {THRESHOLD}\")\n",
    "\n",
    "\n",
    "def hash_func(row):\n",
    "    sum = 0\n",
    "    for e in row[1][0]:\n",
    "        sum += e\n",
    "    return (row[0], (int.from_bytes(hashlib.md5(str(sum).encode()).digest()[:4], byteorder=\"big\"), row[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d876a02-8601-4b6b-940d-f458d84a4cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2, 0), <pyspark.resultiterable.ResultIterable object at 0x7f41e9548250>)\n",
      "((2, 1), <pyspark.resultiterable.ResultIterable object at 0x7f41e9548280>)\n",
      "((2, 2), <pyspark.resultiterable.ResultIterable object at 0x7f41e9548400>)\n",
      "((2, 3), <pyspark.resultiterable.ResultIterable object at 0x7f41e95482b0>)\n",
      "((2, 4), <pyspark.resultiterable.ResultIterable object at 0x7f41e9548490>)\n"
     ]
    }
   ],
   "source": [
    "# returns (doc, band, hash)\n",
    "hash_band_rdd = hash_rdd.flatMap(lambda x: [[(x[0], i % BANDS), hash] for i, hash in enumerate(x[1])]).groupByKey().cache()\n",
    "\n",
    "for elem in hash_band_rdd.take(5):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cc8dd6e-ddde-463a-8916-46359db6e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_bands_grouped_rdd = hash_band_rdd.map(lambda x: [x[0][1], (x[1], x[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eea87054-de57-44d9-ad5a-de7e741324b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2348554175)\n",
      "1153\n",
      "3546\n",
      "(1, 1857684926)\n",
      "1153\n",
      "3546\n",
      "(2, 1968353)\n",
      "1153\n",
      "3546\n",
      "(3, 3586289013)\n",
      "1153\n",
      "3546\n",
      "(4, 837279366)\n",
      "1153\n",
      "3546\n",
      "(5, 3448303943)\n",
      "1153\n",
      "3546\n",
      "(6, 2734432292)\n",
      "1153\n",
      "3546\n",
      "(7, 1327458016)\n",
      "1153\n",
      "3546\n",
      "(8, 1441849978)\n",
      "1153\n",
      "3546\n",
      "(9, 3462345108)\n",
      "1153\n",
      "3546\n"
     ]
    }
   ],
   "source": [
    "band_hashed = hash_bands_grouped_rdd.map(hash_func).map(lambda x: [(x[0], x[1][0]), x[1][1]]).groupByKey().filter(lambda x: (len(x[1]) > 1 and len(x[1]) < 50))\n",
    "\n",
    "for elem in band_hashed.take(10):\n",
    "    print(elem[0])\n",
    "    for b in elem[1]:\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1356de3a-6f5c-4b00-a14b-c8abebc178db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1153, 3546)\n",
      "(1411, 41125)\n",
      "(2168, 5745)\n",
      "(2453, 12251)\n",
      "(2581, 3254)\n",
      "(4640, 4645)\n",
      "(4644, 4700)\n",
      "(5878, 7824)\n",
      "(5885, 30809)\n",
      "(7491, 17511)\n"
     ]
    }
   ],
   "source": [
    "candidates = band_hashed.map(lambda x: tuple(x[1])).distinct()\n",
    "\n",
    "for elem in candidates.take(10):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c9d0cb3-d3a0-4925-a4f1-bbfbced603a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24c8e0dc-4201-4a1a-8e30-b874efdf279f",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 15) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a016283e9e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhash_bands_grouped_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#for h in elem[1]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#    print(h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/9qxx9a49j506x833dcmjlfvqs5bavsw5-python3.9-py4j-0.10.9.2/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/9qxx9a49j506x833dcmjlfvqs5bavsw5-python3.9-py4j-0.10.9.2/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 15) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2234, in combine\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "for elem in hash_bands_grouped_rdd.take(2):\n",
    "    print(elem)\n",
    "    #for h in elem[1]:\n",
    "    #    print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c65ee4-7751-4764-9609-6a54b2973c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 67594442), 2]\n",
      "[(1, 16439677), 2]\n",
      "[(2, 45145227), 2]\n",
      "[(3, 4436395), 2]\n",
      "[(4, 8985140), 2]\n"
     ]
    }
   ],
   "source": [
    "bands_rdd = hash_band_rdd.map(lambda x: [(x[0][1], x[1]), x[0][0]]) \n",
    "\n",
    "for elem in bands_rdd.take(5):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa97640b-712c-4ac2-8ee1-fed261021b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f2f51a374a9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvector_bucket_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbands_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvector_bucket_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/9qxx9a49j506x833dcmjlfvqs5bavsw5-python3.9-py4j-0.10.9.2/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/9qxx9a49j506x833dcmjlfvqs5bavsw5-python3.9-py4j-0.10.9.2/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 242, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "vector_bucket_rdd = bands_rdd.map(lambda x: [x[1], x[0][1]]).distinct()\n",
    "\n",
    "for elem in vector_bucket_rdd.take(1):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd7b97-96fc-436f-ab3f-e5edf7d8457e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048fc5c-8a48-4434-bd6a-92b03ccfdace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f39564-68ac-489f-8700-2b7b56ca7a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958d9c18-eebb-4579-a6d2-987a3d16fce6",
   "metadata": {},
   "source": [
    "# Exit Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903af5b-7751-4eef-a000-1286bdb1eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe907960-4bf9-49a6-b964-910097827ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
