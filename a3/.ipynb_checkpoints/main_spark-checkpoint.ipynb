{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7b45eb-b95e-4bed-b429-10717c3a0512",
   "metadata": {},
   "source": [
    "# Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764bccb-ae89-431d-8b0a-44690d69d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import flatten\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, \n",
    "                                FloatType, DateType, IntegerType, ArrayType)\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"BDA assignment\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534ea9a-b7cf-4971-9330-6cb615863303",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbd6460-830a-445d-863b-b87edb4b0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Final, List\n",
    "#from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import islice, chain, combinations\n",
    "import argparse\n",
    "import traceback\n",
    "import bleach\n",
    "import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import pr\n",
    "import string\n",
    "import random\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6aed4f-9be9-4a14-a996-078adae3e54f",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d055241-e20d-43b1-8406-d412b5f34cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHINGLE_SIZE: Final = 5\n",
    "SAMPLES: Final = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f90724-7f77-4267-bd27-723fdef44ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class comment_tuple(NamedTuple):\n",
    "    id: int\n",
    "    #owner_id: int\n",
    "    post_type: int\n",
    "    score: int\n",
    "    text: str\n",
    "\n",
    "class shingle_set(NamedTuple):\n",
    "    id: int\n",
    "    shingles: frozenset[tuple]\n",
    "\n",
    "class similarity(NamedTuple):\n",
    "    id_set1: int\n",
    "    id_set2: int\n",
    "    similarity: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed988ef-bf90-4ec9-a288-c32a0441df13",
   "metadata": {},
   "source": [
    "# Read and clean XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc472be3-e7df-4983-ad8d-734155857e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_schema():\n",
    "    \"\"\"\n",
    "    Define the schema for the DataFrame\n",
    "    \"\"\"\n",
    "    schema_list = []\n",
    "    schema_list.append(StructField(\"Id\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"PostTypeId\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"Score\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"Body\", StringType(), True))\n",
    "    \n",
    "    return StructType(schema_list)\n",
    "\n",
    "def parse_post(rdd):\n",
    "    results = []\n",
    "    root = ET.fromstring(rdd[0])\n",
    "\n",
    "    for elem in root.findall('row'):\n",
    "        rec = []\n",
    "        #print(\"Found row\")\n",
    "        assert elem.text is None, \"The row wasn't empty\"\n",
    "        rec.append(int(elem.attrib[\"Id\"]))\n",
    "        #int(elem.attrib[\"OwnerUserId\"]),\n",
    "        #rec.append(int(elem.attrib[\"PostTypeId\"])),\n",
    "        #rec.append(int(elem.attrib[\"Score\"])),\n",
    "        rec.append(bleach.clean(elem.attrib[\"Body\"], strip=True))\n",
    "        #rec.append(elem.attrib[\"Body\"])\n",
    "\n",
    "        #elem.clear()\n",
    "        #while elem.getprevious() is not None:\n",
    "        #    del elem.getparent()[0]\n",
    "        results.append(rec)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9e592e-7f1e-40a0-8a94-95a8a8018038",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"cstheory_posts.bakxml\"\n",
    "chunksize = 1024\n",
    "\n",
    "file_rdd = spark.read.text(filename, wholetext=True).rdd\n",
    "dataset = file_rdd.flatMap(parse_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1b1d30-61e5-488f-a09e-76acb8969874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| Id|                Body|\n",
      "+---+--------------------+\n",
      "|  2|I have a dataset ...|\n",
      "|  3|A particular prog...|\n",
      "|  4|What is the follo...|\n",
      "|  5|Can the divide an...|\n",
      "|  6|Is anyone aware o...|\n",
      "|  7|In general, the q...|\n",
      "|  8|If I understand t...|\n",
      "|  9|<a href=\"http://w...|\n",
      "| 10|There was recentl...|\n",
      "| 11|Functional progra...|\n",
      "| 12|I took a class on...|\n",
      "| 13|It's possible tha...|\n",
      "| 14|Other than going ...|\n",
      "| 15|[This question ha...|\n",
      "| 16|See the <a href=\"...|\n",
      "| 17|it is often said ...|\n",
      "| 18|In one word: No.\n",
      "...|\n",
      "| 19|In short, I would...|\n",
      "| 20|It's easy to prov...|\n",
      "| 21|Any &quot;cutting...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.persist()\n",
    "schema = set_schema()\n",
    "df_ds = dataset.toDF(schema)\n",
    "df_ds.show()\n",
    "\n",
    "#df_posts = records_rdd.toDF(schema)\n",
    "#coll = records_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed4fe3-7297-4445-8ce2-32be93b42e1c",
   "metadata": {},
   "source": [
    "# Shingling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e3fecd3-7b72-490d-b4ab-b1c8c4920cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shingler:\n",
    "    \"\"\"\n",
    "    Class that contain a tokenizer and stopwords to make shingling easier.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        return True\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # text_nop = text.split()\n",
    "        text_nop = word_tokenize(text)\n",
    "        filtered_words = []\n",
    "\n",
    "        for word in text_nop:\n",
    "            if word not in self.stopwords:\n",
    "                filtered_words.append(word.lower())\n",
    "        \n",
    "        return filtered_words\n",
    "\n",
    "    #def create_shingle(self, input_comment: comment_tuple, shingle_size: int) -> frozenset[tuple]:\n",
    "    #    tokens = self.tokenize(input_comment.text)\n",
    "    #    comment_length = len(tokens)\n",
    "    #    shingles =  frozenset(tuple(tokens[i:(i + shingle_size)]) for i in range(comment_length - shingle_size + 1))\n",
    "    \n",
    "    def create_shingle(self, post: str, shingle_size: int) -> list[list]:\n",
    "        tokens = self.tokenize(post)\n",
    "        comment_length = len(tokens)\n",
    "        shingle_set = frozenset(tuple(tokens[i:(i + shingle_size)]) for i in range(comment_length - shingle_size + 1))\n",
    "        shingle_list = list(shingle_set)\n",
    "        #for elem in shingle_list:\n",
    "        #    elem.sort()\n",
    "        shingle_list.sort()\n",
    "        return list(shingle_set)\n",
    "        \n",
    "\n",
    "def shingle_map(row):\n",
    "    ds_shingler = shingler()\n",
    "    return (row[0], ds_shingler.create_shingle(row[1], SHINGLE_SIZE)\n",
    ")\n",
    "\n",
    "def set_shingle_schema():\n",
    "    \"\"\"\n",
    "    Define the schema for the DataFrame\n",
    "    \"\"\"\n",
    "    schema_list = []\n",
    "    schema_list.append(StructField(\"Id\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"PostTypeId\", IntegerType(), True))\n",
    "    #schema_list.append(StructField(\"Score\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"Shingles\", ArrayType(ArrayType(StringType()), True)))\n",
    "    return StructType(schema_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "855eb6f2-07ef-4ef9-b1ba-f8d3d0a9209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| Id|            Shingles|\n",
      "+---+--------------------+\n",
      "|  2|[[dataset, linear...|\n",
      "|  3|[[horizontally, v...|\n",
      "|  4|[[problem, k, sub...|\n",
      "|  5|[[chirp, etc, aut...|\n",
      "|  6|[[seems, would, p...|\n",
      "|  7|[[does, yield, pa...|\n",
      "|  8|[[already, used, ...|\n",
      "|  9|[[algebra, descri...|\n",
      "| 10|[[question, might...|\n",
      "| 11|[[equivalent, mat...|\n",
      "| 12|[[classes, r, re,...|\n",
      "| 13|[[hrefhttpmichael...|\n",
      "| 14|[[software, devel...|\n",
      "| 15|[[order, second, ...|\n",
      "| 16|                  []|\n",
      "| 17|[[said, using, md...|\n",
      "| 18|[[built, upon, in...|\n",
      "| 19|[[significantly, ...|\n",
      "| 20|[[ij, grid, diago...|\n",
      "| 21|[[technology, fie...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = set_shingle_schema()\n",
    "shingle_rdd = dataset.map(shingle_map)\n",
    "df_shingle = shingle_rdd.toDF(schema)\n",
    "df_shingle.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b363a0e8-480f-4bc0-9167-1b58be6e6c80",
   "metadata": {},
   "source": [
    "# MinHashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3799fdf-d96c-4f41-87fd-f40c6353a4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2096149490282617182, 2269561747909081125, 607930631362827344, 2092219096079086764, 1809674349804163182, 1338257575307146427, 1928960101259846486, 315389730586719761, 559989775987727879, 1480681506970345963, 2261419810219035797, 2243558520118467258, 1931388090443637456, 2212609764843106243, 350380123870349570, 1304243101527343489, 1592214487878087633, 1277783616767142909, 2280831058830631016, 1360369692187081524, 494790709208836929, 610180859697606999, 2189755580139374733, 480413561091986636, 2161153900773804701, 1002261504377444921, 696261638825223443, 2101683783981916985, 1599279409116898063, 2253616327139758357, 112451947857840231, 102391885042708822, 2213763498531204161, 951137115894174460, 733117990283907917, 106046510276105074, 2004829538990433145, 1098029479313378209, 276824724253688489, 211516063877708228, 2233803146264722951, 2025602953273918832, 142435781221445861, 1284554568422474643, 752807239468083502, 311725101681684327, 171144034284598831, 268901835451830029, 1656869392929775525, 169353549066237318]\n",
      "[[948294967214475975, 383793872832979355, 171691032722129404, 1869207216609245834, 915393617331605620, 2763815642315057, 2190041543119995570, 1958463484549631569, 69761512244248018, 1176418480208199457, 11515514772947187, 417015954502720836, 684977166217459883, 955045061410022707, 2036447499414374350, 1832888334085915425, 1258639412668747927, 1187650147317313007, 505147285417675863, 862085738959634131, 386561151397552462, 2182267671737290510, 603612384365920673, 1418179182882834841, 1968089838237048892, 1426557725947378342, 2139603300186656373, 1358265112566746033, 1982965082807617105, 2287784710218489981, 84549789940954334, 803435780693530676, 2024692381242811011, 1875420938578086237, 340261071829954298, 1273545263244589496, 1922545420014182506, 769284642528947113, 1316901334440279366, 1328174510593391709, 1916005199796406939, 332403008569661165, 475063310303644192, 1492231921610705122, 86935055985473915, 1506373519939636812, 1604572177575277867, 1357494622418693748, 276152729893711750, 1024822996896503038], [2193686009809569506, 186948602053746695, 1305908732402198305, 1844648856581048542, 586025416786808970, 2252454231672961722, 876557755666616048, 1323932675165342657, 1972357378141214695, 1069858384701985009, 1254548805586308869, 1264825373715459037, 1424902843325115973, 1066525827547823987, 1884237990341428623, 513120422706562348, 131197620829569462, 1992302892782469201, 1167527755909296620, 1156547028751355268, 167666539258040833, 9079377590340914, 444205447686296391, 1125167307056469501, 2211287474951557574, 641291314918957014, 392587556630880684, 1816985100136586018, 2184363498369164970, 948999429853586338, 108765397674565185, 329020163655772128, 273754096467510472, 630479389315617991, 2257570367724345858, 1349008710761814879, 1400270595342313793, 789833761681157132, 1428259939813866149, 1592247125081851181, 1082168011636934315, 1721259223607695583, 1618703320774309469, 1488668867535396433, 717165772959083097, 786911873814223439, 1593727335922103096, 1485752908715445156, 278462386319785492, 2098388039738215756]]\n"
     ]
    }
   ],
   "source": [
    "# Transform posts to characteristic matrix\n",
    "# Make feature set matrix\n",
    "# Minhash\n",
    "# Make Minhash Matrix\n",
    "# LSH\n",
    "import random\n",
    "SIGNATURE_SIZE: Final = 50\n",
    "HASH_PRIME: Final = (1 << 61) - 1\n",
    "MAX_HASH: Final = sys.maxsize\n",
    "HASH_RANGE: Final = sys.maxsize\n",
    "SEED: Final = 193120\n",
    "\n",
    "generator = np.random.RandomState(SEED)\n",
    "salts = [generator.randint(1, HASH_PRIME) for _ in range(SIGNATURE_SIZE)]\n",
    "#ermutations = [generator.randint(1, HASH_PRIME) for _ in range(SIGNATURE_SIZE)]\n",
    "\n",
    "permutations = [[generator.randint(1, HASH_PRIME) for _ in range(SIGNATURE_SIZE)],\n",
    "                [generator.randint(0, HASH_PRIME) for _ in range(SIGNATURE_SIZE)]]\n",
    "\n",
    "print(salts)\n",
    "print(permutations)\n",
    "\n",
    "def hash_func(data, salt):\n",
    "    return int.from_bytes(hashlib.md5(int.to_bytes(salt, 8, byteorder=\"big\") + json.dumps(data).encode()).digest()[:8], byteorder=\"big\")\n",
    "    \n",
    "def min_hasher(row):\n",
    "    sig = np.full((SIGNATURE_SIZE), MAX_HASH)\n",
    "    for shingle in row[1]:\n",
    "        for i in range(SIGNATURE_SIZE):\n",
    "            hash_val = hash_func(tuple(shingle), salts[i]) % HASH_PRIME\n",
    "            sig[i] = min(hash_val, sig[i])\n",
    "    return (row[0], sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "863a39cf-392b-4310-9950-92bb58df1890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, array([ 46762968536944061,  91665087464683450,   3213118852271340,\n",
      "        19422336778066634,   3101490556404491, 128758974008313556,\n",
      "        10804686991680359,  50898033533516211,   7345336463719467,\n",
      "        37662988276161198,   4830045840359835,  91800974071367569,\n",
      "          264083379425228,  24762945327514645,  60125358139790662,\n",
      "        26577092998253430, 147297528620741535,  90094138211255532,\n",
      "       174825228843736271,  63862038947365763,  63067262848611878,\n",
      "        14630467159321622,  61431639610174526,  16198796009282300,\n",
      "        20867341287560508,   1143936005560507,   6455270918189900,\n",
      "        34218393967450437,  87572938973122146,   7278583483444263,\n",
      "       191502178004003804,   6937282200197881, 147429445758536684,\n",
      "        66598784701066527,  28009753267180655,  10136328282320085,\n",
      "        78851120080047511,   9667483227534464, 105935228811017669,\n",
      "        86390792287761791,  54223620111278424,  24081312883022476,\n",
      "         7250765836604674,  35139769331736110,  24667146159620381,\n",
      "        57335136409198009,  43129177902687571,   1665748405611529,\n",
      "        42403806133295932,   2042280876502933]))\n",
      "(3, array([ 94332712596924417,   9259132141431811,  13412071487408029,\n",
      "         5950362264717567,   1584288608645665,   5908820581037197,\n",
      "          594038868582309,  21035263748050997,  32825189026350685,\n",
      "        16708958932296497,  17185552940191425,   6338938526782416,\n",
      "        32702305974867783,  32637147491808563,  10519306458233190,\n",
      "        19403173822499884,   1119063596373829,  11123023456179532,\n",
      "        43403427462149916,  10548401932974896,  13727936512476863,\n",
      "        31056968279516454,   5427385077870850,  31669938809070661,\n",
      "        17982704558503336,   3197344318119674,  36567580683048772,\n",
      "        38299061026653398,  27799438336776963,  21572447543012894,\n",
      "        54009097737659019,   2469792037444904,  50027947220513458,\n",
      "        21688590674061862,  44107747940993009,  30378528576779479,\n",
      "        25710653548409200,  77691555671002601, 158073746811720151,\n",
      "        49397662351528447,  30084471251001688,   1379991038162709,\n",
      "        23591012100319316,   8447593660456627,   4690209856873009,\n",
      "         4929217011481278,  16821053145879178,   2176300735120717,\n",
      "        40187427687773935,  32644120573626406]))\n",
      "(4, array([35001698917404599,  3048421296489943,  2962988057911650,\n",
      "        3972028985331099, 75485777385285166, 23308308723749450,\n",
      "       78006686232001210,  1188118662620808,  2147662537147228,\n",
      "       10955501647334520, 77304503740072476,  4519156205599616,\n",
      "       44081743906960865, 29638659774614148,  8087513006534771,\n",
      "       24270326286113542, 70364136524867383, 38195950855908066,\n",
      "        3209484995834124, 18392698005423790, 62114241389075513,\n",
      "        5339214783337666, 36251742353290598,  7990526970589131,\n",
      "       16940634813896563,   822361340706698, 33860418974945841,\n",
      "       14989922653952657,    20516346615521,  1656036797311140,\n",
      "       58564005174547178,  8712864279876888, 26842164683955085,\n",
      "        7799798076390589, 41009830387529091,  9132865434041655,\n",
      "          13823995749762,   276143606823856, 36525486551348661,\n",
      "        1796517196245167, 12373188369561791,  6890091776956199,\n",
      "       25679563408695508, 46994908607398235,  2963048674974593,\n",
      "        8405246081004963,  5022708216814136,  4048172126222321,\n",
      "       30724927073651173, 22014838391263882]))\n"
     ]
    }
   ],
   "source": [
    "hash_rdd = shingle_rdd.map(min_hasher)\n",
    "\n",
    "for elem in hash_rdd.take(3):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d745c9-7084-4756-8d7d-993612a4daf3",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a0a2172-ea98-45a5-bd56-51aef933432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bands: 10, rows 5, threshold 0.6309573444801932\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-26-75dea7093e3a>\", line 7, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-26-75dea7093e3a>\", line 7, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-75dea7093e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhash_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mBANDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhash_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/9qxx9a49j506x833dcmjlfvqs5bavsw5-python3.9-py4j-0.10.9.2/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/9qxx9a49j506x833dcmjlfvqs5bavsw5-python3.9-py4j-0.10.9.2/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-26-75dea7093e3a>\", line 7, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-26-75dea7093e3a>\", line 7, in <lambda>\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "BANDS: Final = 10\n",
    "ROWS: Final = 5\n",
    "THRESHOLD: Final = (1/BANDS) ** (1/ROWS)\n",
    "print(f\"Bands: {BANDS}, rows {ROWS}, threshold {THRESHOLD}\")\n",
    "\n",
    "# returns (doc, band, hash)\n",
    "hash_band_rdd = hash_rdd.flatMap(lambda x: [(x[0], i % BANDS, hash) for i, hash in enumerate(x[1])])\n",
    "\n",
    "for elem in hash_rdd.take(20):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c65ee4-7751-4764-9609-6a54b2973c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97640b-712c-4ac2-8ee1-fed261021b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd7b97-96fc-436f-ab3f-e5edf7d8457e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048fc5c-8a48-4434-bd6a-92b03ccfdace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f39564-68ac-489f-8700-2b7b56ca7a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "958d9c18-eebb-4579-a6d2-987a3d16fce6",
   "metadata": {},
   "source": [
    "# Exit Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903af5b-7751-4eef-a000-1286bdb1eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe907960-4bf9-49a6-b964-910097827ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
