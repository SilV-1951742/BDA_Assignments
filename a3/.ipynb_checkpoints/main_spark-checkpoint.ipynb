{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7b45eb-b95e-4bed-b429-10717c3a0512",
   "metadata": {},
   "source": [
    "# Setup spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7764bccb-ae89-431d-8b0a-44690d69d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, \n",
    "                                FloatType, DateType, IntegerType)\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"BDA assignment\") \\\n",
    "        .setExecutorEnv('PYTHONPATH') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534ea9a-b7cf-4971-9330-6cb615863303",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbd6460-830a-445d-863b-b87edb4b0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Final, List\n",
    "#from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import islice, chain, combinations\n",
    "import argparse\n",
    "import traceback\n",
    "import bleach\n",
    "import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import pr\n",
    "import string\n",
    "import random\n",
    "import hashlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6aed4f-9be9-4a14-a996-078adae3e54f",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d055241-e20d-43b1-8406-d412b5f34cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHINGLE_SIZE: Final = 5\n",
    "SAMPLES: Final = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f90724-7f77-4267-bd27-723fdef44ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class comment_tuple(NamedTuple):\n",
    "    id: int\n",
    "    #owner_id: int\n",
    "    post_type: int\n",
    "    score: int\n",
    "    text: str\n",
    "\n",
    "class shingle_set(NamedTuple):\n",
    "    id: int\n",
    "    shingles: frozenset[tuple]\n",
    "\n",
    "class similarity(NamedTuple):\n",
    "    id_set1: int\n",
    "    id_set2: int\n",
    "    similarity: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed988ef-bf90-4ec9-a288-c32a0441df13",
   "metadata": {},
   "source": [
    "# Read and clean XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc472be3-e7df-4983-ad8d-734155857e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_schema():\n",
    "    \"\"\"\n",
    "    Define the schema for the DataFrame\n",
    "    \"\"\"\n",
    "    schema_list = []\n",
    "    schema_list.append(StructField(\"Id\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"PostTypeId\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"Score\", IntegerType(), True))\n",
    "    schema_list.append(StructField(\"Body\", StringType(), True))\n",
    "    \n",
    "    return StructType(schema_list)\n",
    "\n",
    "def parse_post(rdd):\n",
    "    results = []\n",
    "    root = ET.fromstring(rdd[0])\n",
    "\n",
    "    for elem in root.findall('row'):\n",
    "        rec = []\n",
    "        #print(\"Found row\")\n",
    "        assert elem.text is None, \"The row wasn't empty\"\n",
    "        rec.append(int(elem.attrib[\"Id\"]))\n",
    "        #int(elem.attrib[\"OwnerUserId\"]),\n",
    "        rec.append(int(elem.attrib[\"PostTypeId\"])),\n",
    "        rec.append(int(elem.attrib[\"Score\"])),\n",
    "        rec.append(bleach.clean(elem.attrib[\"Body\"], strip=True))\n",
    "        #rec.append(elem.attrib[\"Body\"])\n",
    "\n",
    "        #elem.clear()\n",
    "        #while elem.getprevious() is not None:\n",
    "        #    del elem.getparent()[0]\n",
    "        results.append(rec)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9e592e-7f1e-40a0-8a94-95a8a8018038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Id,IntegerType,true),StructField(PostTypeId,IntegerType,true),StructField(Score,IntegerType,true),StructField(Body,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "filename = \"cstheory_posts.xml\"\n",
    "chunksize = 1024\n",
    "\n",
    "schema = set_schema()\n",
    "\n",
    "file_rdd = spark.read.text(filename, wholetext=True).rdd\n",
    "records_rdd = file_rdd.flatMap(parse_post)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d1b1d30-61e5-488f-a09e-76acb8969874",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'bleach'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'bleach'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-47db4a2e6dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_posts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecords_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecords_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.231 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'bleach'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 586, in main\n    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\n  File \"/nix/store/4hn9f95hijjfhk93hx3wf87p38l5kbaq-python3.9-pyspark-3.1.1/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 562, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'bleach'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_posts = records_rdd.toDF(schema)\n",
    "coll = records_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55bdfa69-aa19-4170-b1ce-113e8c60e7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I took a class once on Computability and Logic. The material included a correlation between complexity / computability classes (R, RE, co-RE, P, NP, Logspace, ...) and\n",
      "                                 Logics (Predicate calculus, first order logic, ...).\n",
      "\n",
      "The correlation included several results in one fields, that were obtained using techniques from the other field. \n",
      "                                 It was conjectured that P != NP could be attacked as a problem in Logic (by projecting the problem from the domain of complexity classes to logics).\n",
      "                                 \n",
      "\n",
      "Is there a good summary of these techniques and results?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bleach.clean(\"\"\"<p>I took a class once on Computability and Logic. The material included a correlation between complexity / computability classes (R, RE, co-RE, P, NP, Logspace, ...) and\n",
    "                                 Logics (Predicate calculus, first order logic, ...).</p>\\n\\n<p>The correlation included several results in one fields, that were obtained using techniques from the other field. \n",
    "                                 It was conjectured that P != NP could be attacked as a problem in Logic (by projecting the problem from the domain of complexity classes to logics).\n",
    "                                 </p>\\n\\n<p>Is there a good summary of these techniques and results?</p>\\n\"\"\", strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9be53f-8236-4e7b-88e0-09c835953230",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posts = records_rdd.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43eb7992-848d-45df-9317-878f1884e02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Id: int, PostTypeId: int, Score: int, Body: string]\n",
      "[2, 1, 13, '<p>I have a dataset which is a number of objects arranged in a 2-D grid.  I know I have a strict ordering, increasing as you go left-to-right within each row, and increasing as top-to-bottom within each column.  For example, </p>\\n\\n<ul>\\n<li>1 2 3</li>\\n<li>4 6 7</li>\\n<li>5 8 9</li>\\n</ul>\\n\\n<p>Can I improve on naive sorting to sort the entire dataset linearly (as measured in comparisons)?</p>\\n\\n<p>What about for n-d datasets?  Arbitrary finite datasets with a subset of comparisons known?</p>\\n']\n",
      "[3, 1, 8, '<p>A particular programming problem I came across recently reduces to finding hamiltonian paths in a rectangular grid that would look something like,</p>\\n\\n<pre><code>A  0  0  0\\n\\n0  0  0  0\\n\\n0  0  C  D\\n</code></pre>\\n\\n<p>What are some effective heuristics that could be applied to find them - and particularly, techniques to trim/discard paths along the way?</p>\\n\\n<p>Edit: Just to clarify, the edges are formed when elements are connected horizontally and vertically, but not diagonally. The problem also states that any element that is marked 0 can be used to form a path, but non 0 elements are \"obstacles\" that need to be avoided. </p>\\n\\n<pre><code>A-0-0-0\\n      |\\n0-0-0-0\\n|\\n0-0-C D\\n</code></pre>\\n\\n<p>could be one path, for instance. Another may be,</p>\\n\\n<pre><code>A 0-0-0\\n| |   |\\n0 0 0-0\\n| | |\\n0-0 C D\\n</code></pre>\\n']\n",
      "[4, 1, 15, \"<p>What is the following variation on set cover known as?</p>\\n\\n<p>Given a set S, a collection C of subsets of S and a positive integer K, do there exist K sets in C such that every pair of elements of S lies in one of the selected subsets.</p>\\n\\n<p>Note: It is not hard to see that this problem is NP-Complete:  Given a normal set cover problem (S, C, K), make three copies of S, say S', S'', and S''',  then create your subsets as S''', |S| subsets of the form {a'} U {x in S'' | x != a} U {a'''}, |S| subsets of the form {a''} U {x in S' | x != a} U {a'''}, {a', a'' | a in C_i}.  Then we can solve the set cover problem with K subsets iff we can solve the pair cover problem with K + 1 + 2 |S| subsets.</p>\\n\\n<p>This generalizes to triples, etc.  I would like to be able to not waste half a page proving this, and it is probably not obvious enough to dismiss as trivial.  It is certainly sufficiently useful that someone has proved it, but I have no idea who or where.</p>\\n\\n<p>Also, is there a good place to look for NP-Completeness results that are not in Garey and Johnson?</p>\\n\"]\n",
      "[5, 1, 10, \"<p>Can the divide and conquer nature of the FFT be generalized to other transforms (z Transform, chirp, etc) automatically?</p>\\n\\n<p>Is there an algorithm that takes in a description of transform (I don't know what information would be needed) and can produce a fast FFT like function?</p>\\n\"]\n",
      "[6, 1, 8, \"<p>Is anyone aware of any process algebraic (or related) formalisms that capture fine-grained location information?  I'm familiar with ambients and bigraphs, which obviously have a location model, but what about attempts to model more concrete location information, such that the location of some agent is expressed within some (real or virtual) coordinate space?</p>\\n\\n<p>It seems to me that it would be possible to take some existing formalism and enrich it with constructs to model location at some appropriate level of abstraction, but I would hate to reinvent the wheel needlessly if this is reminiscent of previous efforts.</p>\\n\\n<p>As background, the application domain is essentially biological modeling where the traditional notions of location used in this domain to govern where interactions can occur are not fine-grained enough to capture the properties of interest.</p>\\n\"]\n",
      "[7, 1, 21, '<p>In general, the query-tape for an oracle counts towards the space-complexity of a TM. However, it seems plausible to allow a write-only oracle-tape (such as is used in L-space reductions).</p>\\n\\n<p>Is such a construction useful? Does it yield any particularly absurd results?</p>\\n']\n",
      "[8, 2, 4, \"<p>If I understand the problem correctly (and I may not, feel free to tell me if I don't) you want to transform a 2D grid into a sorted 1D array, whereas each row and column is already sorted in the 2D grid?</p>\\n\\n<p>The first element in the list in this case has to be the top-left corner ((0,0), by definition of the problem). After this it has to either be the (1,0) or (0,1) element, as all others will be larger than these by definition.</p>\\n\\n<p>You can generalise by saying that the next smallest element in the grid is always directly below an element already used (or the edge of grid), and also to the right of an element already used (or the edge of grid), since both are defined to be smaller than it.\\nSo at each iteration you must only consider the smallest value that fulfills this requirement.</p>\\n\\n<p>You can keep the possible candidates in sorted order as you find them (no more than two will ever be made available in one iteration), and at each iteration check the new values made available (if any).\\nIf they're lower than the lowest of the previous candidates add them to the list straight away and repeat, otherwise add the lowest previous candidate and compare to the next lowest etc.</p>\\n\\n<p>Unfortunately I don't claim to be able to provide an exact complexity of this, nor do I claim it's the most efficient possible, it certainly seems better than a naive approach, and I hope I explained it well enough for you to understand.</p>\\n\\n<p>EDIT: For n-d grids like this I believe the same basic principle applies, but each iteration makes up to n new candidates available, and these candidates must be the smallest unused elements in each of n dimensions at this point.</p>\\n\"]\n",
      "[9, 2, 7, '<p><a href=\"http://www.doc.ic.ac.uk/~pg/papers/geometric.pdf\" rel=\"noreferrer\">Processes in Space</a> by Luca Cardelli and Philippa Gardner is one paper in this direction. It specified a process algebra for describing the evolution of 3D structures. </p>\\n']\n",
      "[10, 1, 8, '<p>There was recently a claimed <a href=\"http://science.slashdot.org/story/10/08/08/226227/Claimed-Proof-That-P--NP\" rel=\"nofollow\">proof that $P \\\\ne NP$</a>. Not long after its publication there were raised <a href=\"http://slashdot.org/story/10/08/11/0239209/Possible-Issues-With-the-P--NP-Proof\" rel=\"nofollow\">some issues</a> with this proof.</p>\\n\\n<p>So ... is the proof correct or not ? (Please only answer this if you have evidence ... this question might take some time until it\\'s answered)</p>\\n']\n",
      "[11, 1, 50, '<p>Functional programming has a theoretical basis in <a href=\"http://en.wikipedia.org/wiki/Lambda_calculus\">lambda calculus</a> and <a href=\"http://en.wikipedia.org/wiki/Combinatory_logic\">combinatory logic</a>.  As someone involved with statistical computing, I find these concepts to be very useful for modeling.</p>\\n\\n<p>Is there an equivalent mathematical basis of <a href=\"http://en.wikipedia.org/wiki/Imperative_programming\">imperative programming</a>, or did it simply grow out of practical hardware application in machine language and the subsequent development of <a href=\"http://en.wikipedia.org/wiki/FORTRAN\">FORTRAN</a>?</p>\\n']\n",
      "[12, 1, 35, '<p>I took a class once on Computability and Logic. The material included a correlation between complexity / computability classes (R, RE, co-RE, P, NP, Logspace, ...) and Logics (Predicate calculus, first order logic, ...).</p>\\n\\n<p>The correlation included several results in one fields, that were obtained using techniques from the other field. It was conjectured that P != NP could be attacked as a problem in Logic (by projecting the problem from the domain of complexity classes to logics).</p>\\n\\n<p>Is there a good summary of these techniques and results?</p>\\n']\n",
      "[13, 2, 23, '<p>It\\'s possible that you\\'re asking about results in finite model theory (such as the characterization of P and NP in terms of various fragments of logic). The recent attempted proof of P != NP initially made heavy use of such concepts, and some good references (taken from the <a href=\"http://michaelnielsen.org/polymath1/index.php?title=Deolalikar_P_vs_NP_paper\">wiki</a>) are </p>\\n\\n<ul>\\n<li><a href=\"http://www.logic.rwth-aachen.de/pub/graedel/FMTbook-Chapter3.pdf\">Erich Gradel\\'s review of FMT and descriptive complexity</a></li>\\n<li><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=38D2317557C5EF25F3ED6D0D3CBE6E69?doi=10.1.1.28.6917&amp;rep=rep1&amp;type=pdf\">Ron Fagin\\'s article on descriptive complexity</a></li>\\n</ul>\\n']\n",
      "[14, 1, 24, \"<p>Other than going fully academic and getting a doctorate/post-doc, or going for a more or less 'standard' job in software development, what are some other career options in the full or semi theoretical C.S field?</p>\\n\"]\n",
      "[15, 1, 19, '<p>[This question has been asked on MathOverflow with no luck a month ago.]</p>\\n\\n<p>Let me first clarify my definitions.  For a word $w \\\\in \\\\Sigma^*$, with $\\\\Sigma =\\\\{a_1,\\r\\n\\\\ldots, a_n\\\\}$, I define two structures:</p>\\n\\n<p>$\\\\mathbb{N}(w) = \\\\langle \\\\mathbb{N}, &lt;, Q_{a_1}, \\\\ldots, Q_{a_n} \\\\rangle$,</p>\\n\\n<p>and the more usual <i>word model</i>:</p>\\n\\n<p>$\\\\mathbb{N}^r(w) = \\\\langle \\\\{0, \\\\ldots, |w|-1\\\\}, &lt;, Q_{a_1}, \\\\ldots, Q_{a_n} \\\\rangle$,</p>\\n\\n<p>where $Q_{a_i} = \\\\{p \\\\mid w_p = a_i\\\\}$.</p>\\n\\n<p>Then WS1S is the set of second order formulas with models of the form\\n$\\\\mathbb{N}(w)$, with order, and for which second order quantification is\\nlimited to finite subsets of the domain.  MSO is the set of second order\\nformulas with models of the form $\\\\mathbb{N}^r(w)$, with order.</p>\\n\\n<p>The usual proof that REG = WS1S proves at the same time that MSO = WS1S.  My\\nquestion is then, for which first or second order relations can we keep this\\nto be true?</p>\\n\\n<p>For instance, if we add a unary predicate $E(X)$ which says that a\\n(monadic) second order variable contains an even number of objects, we add no\\npower, as $E(X)$ is expressible as \"there exists $X_1$ and\\n$X_2$ that partition $X$, in such a way that if an element is in\\n$X_i$ the next one in $X$ is in $X_j$, $i \\\\neq j$, and\\nthe first element of $X$ is in $X_1$ and the last is in\\n$X_2$.\"</p>\\n\\n<p>Now, if we add a predicate $|X| &lt; |Y|$, then WS1S becomes undecidable\\n(see Klaedtke &amp; Ruess, 10.1.1.7.3029), while MSO stays trivially decidable.</p>\\n\\n<p>Thank you.</p>\\n\\n<hr>\\n\\n<p><b>Edit:</b> As a side question, ... is this question of interest?  I mean, I\\'m no expert in the field, so I\\'m not sure this question is relevant.</p>\\n']\n",
      "[16, 2, 21, '<p>See the <a href=\"http://michaelnielsen.org/polymath1/index.php?title=Deolalikar_P_vs_NP_paper\" rel=\"noreferrer\">wiki</a>. </p>\\n']\n",
      "[17, 1, 16, '<p>it is often said that when using the MD5 algorithm to sign some arbitrary information, the shared secret has to be at the end. Why?</p>\\n']\n",
      "[18, 2, 20, '<p>In one word: No.</p>\\n\\n<p>It seems that there are some fatal flaws in Deolalikar\\'s proposed proof. The real question now is if the \"proof\" has any useful ideas that can be built upon. In any case, it seems that the proof in its current form simply isn\\'t correct, and can\\'t be corrected either. On the other hand, Deolalikar has not given up on the proof, so I guess it\\'s not the end of it.</p>\\n\\n<p>You can find an update here: <a href=\"http://rjlipton.wordpress.com/2010/08/15/the-p%E2%89%A0np-proof-is-one-week-old/\">http://rjlipton.wordpress.com/2010/08/15/the-p%E2%89%A0np-proof-is-one-week-old/</a></p>\\n']\n",
      "[19, 2, 20, '<p>In short, I would say that imperative programming evolved from machine language and programming practice. On the other hand, <a href=\"http://en.wikipedia.org/wiki/Monad_%28functional_programming%29\">monads</a> provide an appropriate semantic framework for describing the semantics of imperative programming language features. The paper   <a href=\"http://www.disi.unige.it/person/MoggiE/ftp/ic91.pdf\"><em>Notions of computation and monads</em></a> by Moggi established the formal foundations. Phil Wadler popularised the idea and contributed significantly to it being the key way of incorporating imperative features into the programming language Haskell. Recent work by Plotkin and Power <a href=\"http://hdl.handle.net/1842/196\"><em>Notions of Computation Determine Monads</em></a> goes the other way stating that some, but not all, notions of (imperative) computation actually give a monad, meaning that in a very essential way monads correspond to imperative (and other) notions of computation.</p>\\n']\n",
      "[20, 2, 19, \"<p>It's easy to prove an Î©(n<sup>2</sup> log n) lower bound on this problem (in the comparison sorting model): if the element at position (i,j) is always within distance 1/2 of i+j, then the grid diagonals are independent of each other, and the sorted order within each grid diagonal is arbitrary. So under this constraint the total number of orderings possible is the product (over all the diagonals of the grid) of the factorials of the lengths of the diagonals, which is exponential in n<sup>2</sup> log n.</p>\\n\\n<p>Which is to say that standard comparison sorting algorithms are asymptotically optimal for grids ordered as you describe.</p>\\n\"]\n",
      "[21, 2, 4, '<p>Any &quot;cutting-edge&quot; technology field will look for theoreticians.</p>\\n<p>One recent example is high-frequency trading, as in this <a href=\"http://cufp.org/jobs/haskellerlangc-developers-high-frequency-trading\" rel=\"nofollow noreferrer\">recent job post</a> from <a href=\"http://www.allstontrading.com/\" rel=\"nofollow noreferrer\">Allston Trading</a> (h.t. to <a href=\"http://twitter.com/donsbot/status/21329927415\" rel=\"nofollow noreferrer\">Don Stewart</a>):</p>\\n<blockquote>\\n<p>Are you familiar with the teachings\\nand lore of SICP, PAIP, TAOCP? Do you\\nknow why monads really are like\\nburritos (or have an even better\\nanalogy)? Do you have a love/hate (or\\njust love?) relationship with\\nHindley-Milner inference? And can you\\nmake code run so fast your CPU begs\\nfor mercy? Then we want to talk to\\nyou.</p>\\n</blockquote>\\n<p>It goes on to guarantee a phone interview to anyone who has solved P != NP (amongst other things).</p>\\n']\n"
     ]
    }
   ],
   "source": [
    "print(df_posts)\n",
    "for i in range(20):\n",
    "    print(coll[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "348b54a2-b66f-49ac-b8c5-e0d7d544563e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d5005c5566c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_posts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/glnpm34s40s7qk2qrjkn3f5lfrpy635q-python3-3.9.4-env/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1205\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nix/store/ph6hpbx4pr31146wpb72yrk3f3yy0xcs-python3-3.9.4/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_posts.write.format(\"csv\").mode(\"overwrite\")\\\n",
    "            .save(\"./output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d9c18-eebb-4579-a6d2-987a3d16fce6",
   "metadata": {},
   "source": [
    "# Exit Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5903af5b-7751-4eef-a000-1286bdb1eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe907960-4bf9-49a6-b964-910097827ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
